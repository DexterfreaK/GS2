{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DexterfreaK/GS2/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgBO8nKn6-ck"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Logistic \"regression\" is classification algorithm. You can use it to classify things:\n",
        "\n",
        "Just to recap, linear regression is simple: you take your data, and plot it on a graph. Then you draw a line that fits your data pretty well. Now you can use that line to predict things.\n",
        "\n",
        "The best line is the one that minimizes a cost. For linear regression, the cost was the distance from each point of data to the line.\n",
        "\n",
        "Similarly, when you are trying to classify something, first you plot your data on a graph.\n",
        "Now you draw a line and assume anything to the left of the line belongs to one class and other half belongs to another class. This line is called a decision boundary, and it splits the data into different classes.\n",
        "\n",
        "\n",
        "In the linear regression example, we predicted a number. For logistic regression, we predict a probability, like \"there's a x% chance that this belongs to one of the class\".\n",
        "\n",
        "In linear regression, the prediction formula looked like this:\n",
        "h<sub>$\\theta$</sub> = $\\theta$<sub>0</sub> + $\\theta$<sub>1</sub>x\n",
        "\n",
        "You would calculate values for $\\theta$<sub>0</sub> and $\\theta$<sub>1</sub>, and then use them to make a prediction.\n",
        "\n",
        "Once you had the optimal values for  $\\theta$<sub>0</sub> and  $\\theta$<sub>1</sub>, the prediction part was easy.\n",
        "\n",
        "We use almost the same formula for logistic regression, with one change:\n",
        "h<sub>$\\theta$</sub>(x) = g($\\theta$<sub>0</sub> + $\\theta$<sub>1</sub>x)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrN4vFLo6-cm"
      },
      "source": [
        "The g(...) is called the sigmoid function. This function is what makes the prediction function output a percentage. The sigmoid function constrains the result to between 0 and 1.\n",
        "\n",
        "The sigmoid function is:\n",
        "\n",
        "g(z) = $\\frac{1}{1 + e^{-x}}$. Just like linear regression, we will use a cost function to find good values for $\\theta$<sub>0</sub> and $\\theta$<sub>1</sub>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_VzBaLh6-cn"
      },
      "source": [
        "The cost was based on how far off the line was from our data points.\n",
        "\n",
        "In logistic regression, the cost again depends on how far off our predictions are from our actual data. But we are using percentages, so the cost is calculated a little differently i.e. we calculate it using a log scale.\n",
        "\n",
        "Cost function of linear regression is:\n",
        "\n",
        "J($\\theta$<sub>0</sub> + $\\theta$<sub>1</sub>x) = ${\\frac{1}{2m}}$  $\\sum_{i=1}^{m} ((h_\\theta(x_i) - (y_i))^2$\n",
        "\n",
        "However, the cost function of logistic regression is:\n",
        "\n",
        "J($\\theta$) = -${\\frac{1}{m}}$  $\\sum_{i=1}^{m} y_i.log(h_\\theta(x_i)) + (1-y_i).log(1-h_\\theta(x_i))$.\n",
        "This works because one of those two will always be zero, so only the other one will get used. i.e.\n",
        "\n",
        "<br>\n",
        "$y_i.log(h_\\theta(x_i))$ = 0 if $y_i = 0$                  <br>\n",
        "or\n",
        "<br>\n",
        "$(1-y_i).log(1-h_\\theta(x_i))$ = 1 if $y_i = 1$.\n",
        "\n",
        "And then we still use gradient descent to iteratively find the correct values for $\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_I_aKHo6-cn"
      },
      "source": [
        "### Logistic Regression with Newton's 2nd Order optimization Method as well as the regular batch gradient descent.\n",
        "\n",
        "We will be using the Breast Cancer Wisconsin data Set to classify if a tumour is malignant or benign, based on 30 features, such as the mean radius."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbS7kjro6-co"
      },
      "outputs": [],
      "source": [
        "## Dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fvgVAaF6-cp"
      },
      "source": [
        "## Data Preparation\n",
        "We will begin by doing some simple data cleaning and preparation before delving into the logistic regression and newton's method formulas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vMEkfbP6-cp"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "## Dropping a unused fields\n",
        "fields_to_drop = ['id', 'Unnamed: 32']\n",
        "data = data.drop(fields_to_drop, axis=1)\n",
        "\n",
        "## Converting diagnosis to int - 1 for malignant, 0 - for benign\n",
        "d = {'M': 1, 'B': 0}\n",
        "data['diagnosis'] = data['diagnosis'].map(d)\n",
        "\n",
        "## Visualising the data set\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uspz6TNo6-cq"
      },
      "outputs": [],
      "source": [
        "## Using 10% of dataset for testing, 10% for Validation\n",
        "test_split_idx = int(data.shape[0]*0.9)\n",
        "val_split_idx = int(data.shape[0]*0.8)\n",
        "\n",
        "test_data = data[test_split_idx:]\n",
        "val_data = data[val_split_idx:test_split_idx]\n",
        "data = data[:val_split_idx]\n",
        "\n",
        "## Separating data to features and targets\n",
        "train_Y, train_X = data['diagnosis'], data.drop('diagnosis', axis=1)\n",
        "val_Y, val_X = val_data['diagnosis'], val_data.drop('diagnosis', axis=1)\n",
        "test_Y, test_X = test_data['diagnosis'], test_data.drop('diagnosis', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW25UooH6-cq"
      },
      "outputs": [],
      "source": [
        "train_X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_5pMrVc6-cr"
      },
      "source": [
        "## Logistic Regression\n",
        "Logistic Regression is a method in machine learning for classification problems to output discrete values. For example, given an input of the tumour size, the logistic function can classify it as malignant (1) or benign (0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDXD5lgl6-cr"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0stl9376-cr"
      },
      "source": [
        "## Newton's Method\n",
        "Newton's method is a second-order optimization algorithm that can help us find the best weights in our logistic function in fewer iterations compared to batch gradient descent.\n",
        "\n",
        "The generalization of Newton’s method to a multidimensional setting (also called the Newton-Raphson method) is given by:\n",
        "![](images/newton.png)\n",
        "\n",
        "For Logistic Regression, the Hessian is given by:\n",
        "\n",
        "$$\n",
        "Hf(\\beta) = -X^TWX\n",
        "$$\n",
        "and the gradient is:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\beta) = X^T(y-p)\n",
        "$$\n",
        "where\n",
        "$$\n",
        "W := \\text{diag}\\left(p(1-p)\\right)\n",
        "$$  \n",
        "and $p$ are the predicted probabilites computed at the current value of $\\beta$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLOWUrHU6-cr"
      },
      "outputs": [],
      "source": [
        "def newton_step(curr, y, X, reg=None):\n",
        "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T  # probability matrix - N x 1\n",
        "    W = np.diag((p*(1-p))[:,0]) # N by N diagonal matrix\n",
        "    hessian = X.T.dot(W).dot(X)  # 30 by 30 matrix\n",
        "    grad = X.T.dot(y-p)  # 30 by 1 matrix\n",
        "\n",
        "    # regularization step\n",
        "    if reg:\n",
        "        step = np.dot(np.linalg.inv(hessian + reg*np.eye(curr.shape[0])), grad)\n",
        "    else:\n",
        "        step = np.dot(np.linalg.inv(hessian), grad)\n",
        "\n",
        "    beta = curr + step\n",
        "\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-S01iYI6-cs"
      },
      "outputs": [],
      "source": [
        "def check_convergence(beta_old, beta_new, tol, iters):\n",
        "    coef_change = np.abs(beta_old - beta_new)\n",
        "    return not (np.any(coef_change>tol) and iters < max_iters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfyJbbw56-cs"
      },
      "outputs": [],
      "source": [
        "def test_model(X, y, beta):\n",
        "    prob = np.array(sigmoid(X.dot(beta)))\n",
        "\n",
        "    ## Converting prob to prediction, >.5 = True, <.5 = False\n",
        "    prob = np.greater(prob, 0.5*np.ones((prob.shape[1],1)))\n",
        "    accuracy = np.count_nonzero(np.equal(prob, y))/prob.shape[0] * 100\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFbRiUjS6-cs"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As_EnMH46-cs"
      },
      "outputs": [],
      "source": [
        "## Hyperparameters\n",
        "max_iters = 20\n",
        "tol=0.1 # convergence tolerance\n",
        "reg_term = 1\n",
        "\n",
        "beta_old, beta = np.ones((30,1)), np.zeros((30,1))\n",
        "iter_count = 0\n",
        "coefs_converged = False\n",
        "\n",
        "while not coefs_converged:\n",
        "    print('Iteration: {}'.format(iter_count))\n",
        "    print('Validation Accuracy: {}%'.format(\n",
        "        test_model(val_X, val_Y.to_frame(), beta_old)))\n",
        "    beta_old = beta\n",
        "    beta = newton_step(beta, train_Y.to_frame(), train_X, reg_term)\n",
        "    iter_count += 1\n",
        "    coefs_converged = check_convergence(beta_old, beta, tol, iter_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyWlk6O76-cs"
      },
      "source": [
        "### Testing\n",
        "As observed, using Newton's Method, the model converges in very few number of iterations and achieves a high test accuracy of 96%. Typically, Newton's method enjoys faster convergence than batch gradient descent, but each iteration takes long as finding and inverting the Hessian is computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI_5tOGc6-cs"
      },
      "outputs": [],
      "source": [
        "print('After {} Iterations'.format(iter_count))\n",
        "print('Test Accuracy: {}%'.format(\n",
        "        test_model(test_X, test_Y.to_frame(), beta)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo3oFYoV6-cs"
      },
      "outputs": [],
      "source": [
        "beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvcBNjCf6-cs"
      },
      "source": [
        "## Batch Gradient Ascent\n",
        "To optimize the beta values of the logistic function, we can also use the following gradient ascent update rule:\n",
        "![](images/sgd.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LT4JMO76-ct"
      },
      "outputs": [],
      "source": [
        "def gd_step(curr, y, X, lr=0.0000001):\n",
        "    hx = X.dot(curr)\n",
        "    p = np.array(sigmoid(hx))\n",
        "    change = lr * (X.T.dot(y-p))\n",
        "    beta = curr + change\n",
        "\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi_07Lbn6-ct"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 50\n",
        "lr = 0.0001\n",
        "max_iters = 51\n",
        "\n",
        "beta_old, beta = np.ones((30,1)), np.zeros((30,1))\n",
        "iter_count = 0\n",
        "\n",
        "while iter_count < max_iters:\n",
        "    if iter_count % 10 == 0:\n",
        "        print('Epoch: {}'.format(iter_count))\n",
        "        print('Validation Accuracy: {}%'.format(\n",
        "            test_model(val_X, val_Y.to_frame(), beta)))\n",
        "    beta_old = beta\n",
        "    for i in range(0, train_X.shape[0], batch_size):\n",
        "        beta = gd_step(beta, train_Y[i:i+batch_size].to_frame(),\n",
        "                        train_X[i:i+batch_size], lr)\n",
        "    iter_count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAXKkH8n6-ct"
      },
      "source": [
        "### Testing\n",
        "While  batch gradient ascent achieves a similar accuracy of 94% after 100 iterations, a learning rate has to be determined alongside with the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouSHo8CN6-ct"
      },
      "outputs": [],
      "source": [
        "print('After {} Iterations'.format(iter_count))\n",
        "print('Test Accuracy: {}%'.format(\n",
        "        test_model(test_X, test_Y.to_frame(), beta)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUxPRhik6-ct"
      },
      "outputs": [],
      "source": [
        "beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpQ5Qtmb6-cv"
      },
      "source": [
        "###  Softmax Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I1xAj2b6-cv"
      },
      "source": [
        "*Softmax Regression* (synonyms: *Multinomial Logistic*, *Maximum Entropy Classifier*, or just *Multi-class Logistic Regression*) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are  mutually exclusive). In contrast, we use the (standard) *Logistic Regression* model in binary classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aav-nPds6-cv"
      },
      "source": [
        "In *Softmax Regression* (SMR), we replace the sigmoid logistic function by the so-called *softmax* function $\\phi_{softmax}(\\cdot)$.\n",
        "\n",
        "$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}},$$\n",
        "\n",
        "where we define the net input *z* as\n",
        "\n",
        "$$z = w_1x_1 + ... + w_mx_m  + b= \\sum_{l=0}^{m} w_l x_l + b= \\mathbf{w}^T\\mathbf{x} + b.$$\n",
        "\n",
        "(**w** is the weight vector, $\\mathbf{x}$ is the feature vector of 1 training sample, and $b$ is the bias unit.)   \n",
        "Now, this softmax function computes the probability that this training sample $\\mathbf{x}^{(i)}$ belongs to class $j$ given the weight and net input $z^{(i)}$. So, we compute the probability $p(y = j \\mid \\mathbf{x^{(i)}; w}_j)$ for each class label in  $j = 1, \\ldots, k.$. Note the normalization term in the denominator which causes these class probabilities to sum up to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wKX7QAZ6-cv"
      },
      "source": [
        "To illustrate the concept of softmax, let us walk through a concrete example. Let's assume we have a training set consisting of 4 samples from 3 different classes (0, 1, and 2)\n",
        "\n",
        "- $x_0 \\rightarrow \\text{class }0$\n",
        "- $x_1 \\rightarrow \\text{class }1$\n",
        "- $x_2 \\rightarrow \\text{class }2$\n",
        "- $x_3 \\rightarrow \\text{class }2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xCMMlr_I6-cv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y = np.array([0, 1, 2, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaZVZXBG6-cw"
      },
      "source": [
        "First, we want to encode the class labels into a format that we can more easily work with; we apply one-hot encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fkrL8GK6-cw"
      },
      "outputs": [],
      "source": [
        "y_enc = (np.arange(np.max(y) + 1) == y[:, None]).astype(float)\n",
        "print('one-hot encoding:\\n', y_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s7rstwn6-cw"
      },
      "source": [
        "A sample that belongs to class 0 (the first row) has a 1 in the first cell, a sample that belongs to class 2 has a 1 in the second cell of its row, and so forth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2j2gecT6-cw"
      },
      "source": [
        "Next, let us define the feature matrix of our 4 training samples. Here, we assume that our dataset consists of 2 features; thus, we create a 4x2 dimensional matrix of our samples and features.\n",
        "Similarly, we create a 2x3 dimensional weight matrix (one row per feature and one column for each class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReoIDUVA6-cw"
      },
      "outputs": [],
      "source": [
        "X = np.array([[0.1, 0.5],\n",
        "              [1.1, 2.3],\n",
        "              [-1.1, -2.3],\n",
        "              [-1.5, -2.5]])\n",
        "\n",
        "W = np.array([[0.1, 0.2, 0.3],\n",
        "              [0.1, 0.2, 0.3]])\n",
        "\n",
        "bias = np.array([0.01, 0.1, 0.1])\n",
        "\n",
        "print('Inputs X:\\n', X)\n",
        "print('\\nWeights W:\\n', W)\n",
        "print('\\nbias:\\n', bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhgG3ZUg6-c1"
      },
      "source": [
        "To compute the net input, we multiply the 4x2 matrix feature matrix `X` with the 2x3 (n_features x n_classes) weight matrix `W`, which yields a 4x3 output matrix (n_samples x n_classes) to which we then add the bias unit:\n",
        "\n",
        "$$\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-jf5cTC6-c1"
      },
      "outputs": [],
      "source": [
        "X = np.array([[0.1, 0.5],\n",
        "              [1.1, 2.3],\n",
        "              [-1.1, -2.3],\n",
        "              [-1.5, -2.5]])\n",
        "\n",
        "W = np.array([[0.1, 0.2, 0.3],\n",
        "              [0.1, 0.2, 0.3]])\n",
        "\n",
        "bias = np.array([0.01, 0.1, 0.1])\n",
        "\n",
        "print('Inputs X:\\n', X)\n",
        "print('\\nWeights W:\\n', W)\n",
        "print('\\nbias:\\n', bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doLAGgRI6-c1"
      },
      "outputs": [],
      "source": [
        "def net_input(X, W, b):\n",
        "    return (X.dot(W) + b)\n",
        "\n",
        "net_in = net_input(X, W, bias)\n",
        "print('net input:\\n', net_in)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceh7j0Dv6-c2"
      },
      "source": [
        "Now, it's time to compute the softmax activation that we discussed earlier:\n",
        "\n",
        "$$P(y=j \\mid z^{(i)}) = \\phi_{softmax}(z^{(i)}) = \\frac{e^{z^{(i)}}}{\\sum_{j=0}^{k} e^{z_{k}^{(i)}}}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZrB56vE6-c2"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
        "\n",
        "smax = softmax(net_in)\n",
        "print('softmax:\\n', smax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O3rKScM6-c2"
      },
      "source": [
        "As we can see, the values for each sample (row) nicely sum up to 1 now. E.g., we can say that the first sample   \n",
        "`[ 0.29450637  0.34216758  0.36332605]` has a 29.45% probability to belong to class 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXxuMjCj6-c2"
      },
      "source": [
        "Now, in order to turn these probabilities back into class labels, we could simply take the argmax-index position of each row:\n",
        "\n",
        "[[ 0.29450637  0.34216758  **0.36332605**] -> 2   \n",
        "[ 0.21290077  0.32728332  **0.45981591**]  -> 2  \n",
        "[ **0.42860913**  0.33380113  0.23758974]  -> 0  \n",
        "[ **0.44941979**  0.32962558  0.22095463]] -> 0  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep4X_OgF6-c2"
      },
      "outputs": [],
      "source": [
        "def to_classlabel(z):\n",
        "    return z.argmax(axis=1)\n",
        "\n",
        "print('predicted class labels: ', to_classlabel(smax))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJcOXFvF6-c3"
      },
      "source": [
        "As we can see, our predictions are terribly wrong, since the correct class labels are `[0, 1, 2, 2]`. Now, in order to train our logistic model (e.g., via an optimization algorithm such as gradient descent), we need to define a cost function $J(\\cdot)$ that we want to minimize:\n",
        "\n",
        "$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i),$$\n",
        "\n",
        "which is the average of all cross-entropies over our $n$ training samples. The cross-entropy  function is defined as\n",
        "\n",
        "$$H(T_i, O_i) = -\\sum_m T_i \\cdot log(O_i).$$\n",
        "\n",
        "Here the $T$ stands for \"target\" (i.e., the *true* class labels) and the $O$ stands for output -- the computed *probability* via softmax; **not** the predicted class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5EHtNli6-c3"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(output, y_target):\n",
        "    return - np.sum(np.log(output) * (y_target), axis=1)\n",
        "\n",
        "xent = cross_entropy(smax, y_enc)\n",
        "print('Cross Entropy:', xent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5coGv5g6-c3"
      },
      "outputs": [],
      "source": [
        "def cost(output, y_target):\n",
        "    return np.mean(cross_entropy(output, y_target))\n",
        "\n",
        "J_cost = cost(smax, y_enc)\n",
        "print('Cost: ', J_cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsWKlQr56-c3"
      },
      "source": [
        "In order to learn our softmax model -- determining the weight coefficients -- via gradient descent, we then need to compute the derivative\n",
        "\n",
        "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}).$$\n",
        "\n",
        "I don't want to walk through the tedious details here, but this cost derivative turns out to be simply:\n",
        "\n",
        "$$\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum^{n}_{i=0} \\big[\\mathbf{x}^{(i)}\\ \\big(O_i - T_i \\big) \\big]$$\n",
        "\n",
        "We can then use the cost derivate to update the weights in opposite direction of the cost gradient with learning rate $\\eta$:\n",
        "\n",
        "$$\\mathbf{w}_j := \\mathbf{w}_j - \\eta \\nabla \\mathbf{w}_j \\, J(\\mathbf{W}; \\mathbf{b})$$\n",
        "\n",
        "for each class $$j \\in \\{0, 1, ..., k\\}$$\n",
        "\n",
        "(note that $\\mathbf{w}_j$ is the weight vector for the class $y=j$), and we update the bias units\n",
        "\n",
        "\n",
        "$$\\mathbf{b}_j := \\mathbf{b}_j   - \\eta \\bigg[ \\frac{1}{n} \\sum^{n}_{i=0} \\big(O_i - T_i  \\big) \\bigg].$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-wDyLxU6-c3"
      },
      "source": [
        "As a penalty against complexity, an approach to reduce the variance of our model and decrease the degree of overfitting by adding additional bias, we can further add a regularization term such as the L2 term with the regularization parameter $\\lambda$:\n",
        "    \n",
        "L2:        $\\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$,\n",
        "\n",
        "where\n",
        "\n",
        "$$||\\mathbf{w}||_{2}^{2} = \\sum^{m}_{l=0} \\sum^{k}_{j=0} w_{i, j}$$\n",
        "\n",
        "so that our cost function becomes\n",
        "\n",
        "$$J(\\mathbf{W}; \\mathbf{b}) = \\frac{1}{n} \\sum_{i=1}^{n} H(T_i, O_i) + \\frac{\\lambda}{2} ||\\mathbf{w}||_{2}^{2}$$\n",
        "\n",
        "and we define the \"regularized\" weight update as\n",
        "\n",
        "$$\\mathbf{w}_j := \\mathbf{w}_j -  \\eta \\big[\\nabla \\mathbf{w}_j \\, J(\\mathbf{W}) + \\lambda \\mathbf{w}_j \\big].$$\n",
        "\n",
        "(Please note that we don't regularize the bias term.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKBqTa1F6-c3"
      },
      "source": [
        "# Softmax Regression Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfJOgX326-c3"
      },
      "source": [
        "Bringing the concepts together, we could come up with an implementation as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2aYiMxr6-c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "#from .._base import _BaseClassifier\n",
        "#from .._base import _BaseMultiClass\n",
        "\n",
        "\n",
        "class SoftmaxRegression(object):\n",
        "\n",
        "    \"\"\"Softmax regression classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float (default: 0.01)\n",
        "        Learning rate (between 0.0 and 1.0)\n",
        "    epochs : int (default: 50)\n",
        "        Passes over the training dataset.\n",
        "        Prior to each epoch, the dataset is shuffled\n",
        "        if `minibatches > 1` to prevent cycles in stochastic gradient descent.\n",
        "    l2 : float\n",
        "        Regularization parameter for L2 regularization.\n",
        "        No regularization if l2=0.0.\n",
        "    minibatches : int (default: 1)\n",
        "        The number of minibatches for gradient-based optimization.\n",
        "        If 1: Gradient Descent learning\n",
        "        If len(y): Stochastic Gradient Descent (SGD) online learning\n",
        "        If 1 < minibatches < len(y): SGD Minibatch learning\n",
        "    n_classes : int (default: None)\n",
        "        A positive integer to declare the number of class labels\n",
        "        if not all class labels are present in a partial training set.\n",
        "        Gets the number of class labels automatically if None.\n",
        "    random_seed : int (default: None)\n",
        "        Set random state for shuffling and initializing the weights.\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 2d-array, shape={n_features, 1}\n",
        "      Model weights after fitting.\n",
        "    b_ : 1d-array, shape={1,}\n",
        "      Bias unit after fitting.\n",
        "    cost_ : list\n",
        "        List of floats, the average cross_entropy for each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, epochs=50,\n",
        "                 l2=0.0,\n",
        "                 minibatches=1,\n",
        "                 n_classes=None,\n",
        "                 random_seed=None):\n",
        "\n",
        "        self.eta = eta\n",
        "        self.epochs = epochs\n",
        "        self.l2 = l2\n",
        "        self.minibatches = minibatches\n",
        "        self.n_classes = n_classes\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def _fit(self, X, y, init_params=True):\n",
        "        if init_params:\n",
        "            if self.n_classes is None:\n",
        "                self.n_classes = np.max(y) + 1\n",
        "            self._n_features = X.shape[1]\n",
        "\n",
        "            self.b_, self.w_ = self._init_params(\n",
        "                weights_shape=(self._n_features, self.n_classes),\n",
        "                bias_shape=(self.n_classes,),\n",
        "                random_seed=self.random_seed)\n",
        "            self.cost_ = []\n",
        "\n",
        "        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            for idx in self._yield_minibatches_idx(\n",
        "                    n_batches=self.minibatches,\n",
        "                    data_ary=y,\n",
        "                    shuffle=True):\n",
        "                # givens:\n",
        "                # w_ -> n_feat x n_classes\n",
        "                # b_  -> n_classes\n",
        "\n",
        "                # net_input, softmax and diff -> n_samples x n_classes:\n",
        "                net = self._net_input(X[idx], self.w_, self.b_)\n",
        "                softm = self._softmax(net)\n",
        "                diff = softm - y_enc[idx]\n",
        "                mse = np.mean(diff, axis=0)\n",
        "\n",
        "                # gradient -> n_features x n_classes\n",
        "                grad = np.dot(X[idx].T, diff)\n",
        "\n",
        "                # update in opp. direction of the cost gradient\n",
        "                self.w_ -= (self.eta * grad +\n",
        "                            self.eta * self.l2 * self.w_)\n",
        "                self.b_ -= (self.eta * np.sum(diff, axis=0))\n",
        "\n",
        "            # compute cost of the whole epoch\n",
        "            net = self._net_input(X, self.w_, self.b_)\n",
        "            softm = self._softmax(net)\n",
        "            cross_ent = self._cross_entropy(output=softm, y_target=y_enc)\n",
        "            cost = self._cost(cross_ent)\n",
        "            self.cost_.append(cost)\n",
        "        return self\n",
        "\n",
        "    def fit(self, X, y, init_params=True):\n",
        "        \"\"\"Learn model from training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "            Target values.\n",
        "        init_params : bool (default: True)\n",
        "            Re-initializes model parametersprior to fitting.\n",
        "            Set False to continue training with weights from\n",
        "            a previous model fitting.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        if self.random_seed is not None:\n",
        "            np.random.seed(self.random_seed)\n",
        "        self._fit(X=X, y=y, init_params=init_params)\n",
        "        self._is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def _predict(self, X):\n",
        "        probas = self.predict_proba(X)\n",
        "        return self._to_classlabels(probas)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict targets from X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        target_values : array-like, shape = [n_samples]\n",
        "          Predicted target values.\n",
        "\n",
        "        \"\"\"\n",
        "        if not self._is_fitted:\n",
        "            raise AttributeError('Model is not fitted, yet.')\n",
        "        return self._predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities of X from the net input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        Class probabilties : array-like, shape= [n_samples, n_classes]\n",
        "\n",
        "        \"\"\"\n",
        "        net = self._net_input(X, self.w_, self.b_)\n",
        "        softm = self._softmax(net)\n",
        "        return softm\n",
        "\n",
        "    def _net_input(self, X, W, b):\n",
        "        return (X.dot(W) + b)\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
        "\n",
        "    def _cross_entropy(self, output, y_target):\n",
        "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
        "\n",
        "    def _cost(self, cross_entropy):\n",
        "        L2_term = self.l2 * np.sum(self.w_ ** 2)\n",
        "        cross_entropy = cross_entropy + L2_term\n",
        "        return 0.5 * np.mean(cross_entropy)\n",
        "\n",
        "    def _to_classlabels(self, z):\n",
        "        return z.argmax(axis=1)\n",
        "\n",
        "    def _init_params(self, weights_shape, bias_shape=(1,), dtype='float64',\n",
        "                     scale=0.01, random_seed=None):\n",
        "        \"\"\"Initialize weight coefficients.\"\"\"\n",
        "        if random_seed:\n",
        "            np.random.seed(random_seed)\n",
        "        w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n",
        "        b = np.zeros(shape=bias_shape)\n",
        "        return b.astype(dtype), w.astype(dtype)\n",
        "\n",
        "    def _one_hot(self, y, n_labels, dtype):\n",
        "        \"\"\"Returns a matrix where each sample in y is represented\n",
        "           as a row, and each column represents the class label in\n",
        "           the one-hot encoding scheme.\n",
        "\n",
        "        Example:\n",
        "\n",
        "            y = np.array([0, 1, 2, 3, 4, 2])\n",
        "            mc = _BaseMultiClass()\n",
        "            mc._one_hot(y=y, n_labels=5, dtype='float')\n",
        "\n",
        "            np.array([[1., 0., 0., 0., 0.],\n",
        "                      [0., 1., 0., 0., 0.],\n",
        "                      [0., 0., 1., 0., 0.],\n",
        "                      [0., 0., 0., 1., 0.],\n",
        "                      [0., 0., 0., 0., 1.],\n",
        "                      [0., 0., 1., 0., 0.]])\n",
        "\n",
        "        \"\"\"\n",
        "        mat = np.zeros((len(y), n_labels))\n",
        "        for i, val in enumerate(y):\n",
        "            mat[i, val] = 1\n",
        "        return mat.astype(dtype)\n",
        "\n",
        "    def _yield_minibatches_idx(self, n_batches, data_ary, shuffle=True):\n",
        "            indices = np.arange(data_ary.shape[0])\n",
        "\n",
        "            if shuffle:\n",
        "                indices = np.random.permutation(indices)\n",
        "            if n_batches > 1:\n",
        "                remainder = data_ary.shape[0] % n_batches\n",
        "\n",
        "                if remainder:\n",
        "                    minis = np.array_split(indices[:-remainder], n_batches)\n",
        "                    minis[-1] = np.concatenate((minis[-1],\n",
        "                                                indices[-remainder:]),\n",
        "                                               axis=0)\n",
        "                else:\n",
        "                    minis = np.array_split(indices, n_batches)\n",
        "\n",
        "            else:\n",
        "                minis = (indices,)\n",
        "\n",
        "            for idx_batch in minis:\n",
        "                yield idx_batch\n",
        "\n",
        "    def _shuffle_arrays(self, arrays):\n",
        "        \"\"\"Shuffle arrays in unison.\"\"\"\n",
        "        r = np.random.permutation(len(arrays[0]))\n",
        "        return [ary[r] for ary in arrays]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz7mbz4b6-c4"
      },
      "source": [
        "## Example 1 - Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHrHLZez6-c4"
      },
      "outputs": [],
      "source": [
        "from mlxtend.data import iris_data\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loading Data\n",
        "\n",
        "X, y = iris_data()\n",
        "X = X[:, [0, 3]] # sepal length and petal width\n",
        "\n",
        "# standardize\n",
        "X[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
        "X[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
        "\n",
        "lr = SoftmaxRegression(eta=0.01, epochs=10, minibatches=1, random_seed=0)\n",
        "lr.fit(X, y)\n",
        "\n",
        "plot_decision_regions(X, y, clf=lr)\n",
        "plt.title('Softmax Regression - Gradient Descent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(lr.cost_)), lr.cost_)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBE5Lo7G6-c4"
      },
      "source": [
        "Continue training for another 800 epochs by calling the `fit` method with `init_params=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5vfs4nZ6-c5"
      },
      "outputs": [],
      "source": [
        "lr.epochs = 800\n",
        "\n",
        "lr.fit(X, y, init_params=False)\n",
        "\n",
        "plot_decision_regions(X, y, clf=lr)\n",
        "plt.title('Softmax Regression - Stochastic Gradient Descent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(lr.cost_)), lr.cost_)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VMVQgTih6-c5"
      },
      "source": [
        "### Predicting Class Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79MyaTy96-c5"
      },
      "outputs": [],
      "source": [
        "y_pred = lr.predict(X)\n",
        "print('Last 3 Class Labels: %s' % y_pred[-3:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBM0QEz76-c5"
      },
      "source": [
        "### Predicting Class Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5i0kS2R6-c5"
      },
      "outputs": [],
      "source": [
        "y_pred = lr.predict_proba(X)\n",
        "print('Last 3 Class Labels:\\n %s' % y_pred[-3:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHUyu44o6-c6"
      },
      "source": [
        "## Example 2 - Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca5Da5Vt6-c6"
      },
      "outputs": [],
      "source": [
        "from mlxtend.data import iris_data\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from mlxtend.classifier import SoftmaxRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loading Data\n",
        "\n",
        "X, y = iris_data()\n",
        "X = X[:, [0, 3]] # sepal length and petal width\n",
        "\n",
        "# standardize\n",
        "X[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()\n",
        "X[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()\n",
        "\n",
        "lr = SoftmaxRegression(eta=0.05, epochs=200, minibatches=len(y), random_seed=0)\n",
        "lr.fit(X, y)\n",
        "\n",
        "plot_decision_regions(X, y, clf=lr)\n",
        "plt.title('Softmax Regression - Stochastic Gradient Descent')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(lr.cost_)), lr.cost_)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**"
      ],
      "metadata": {
        "id": "4o0Av8WY4l6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load palmerpenguin dataset which contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n",
        "\n",
        "In the dataset, culmen is the upper ridge of a bird’s bill. In the simplified penguins data, culmen length and depth are renamed as variables bill_length_mm and bill_depth_mm to be more intuitive.\n",
        "\n",
        "Firstly, perform the required preprocessing and then apply the logistic regression technique. Then train logistic model with different optimization algorithm by minimizing errors between predicted and actual results and compare the result."
      ],
      "metadata": {
        "id": "-IUZ7mUk-Jdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('penguin.csv')\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "NFkzUo6Bj5m5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fb6330-3e35-4680-904d-cafd3faebe32"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "island                0\n",
            "bill_length_mm        2\n",
            "bill_depth_mm         2\n",
            "flipper_length_mm     2\n",
            "body_mass_g           2\n",
            "sex                  11\n",
            "species               0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode 'species' using LabelEncoder\n",
        "le_species = LabelEncoder()\n",
        "df['species'] = le_species.fit_transform(df['species'])\n",
        "\n",
        "# Encode 'sex' using LabelEncoder\n",
        "le_sex = LabelEncoder()\n",
        "df['sex'] = le_sex.fit_transform(df['sex'])\n",
        "\n",
        "# One-hot encode 'island'\n",
        "df = pd.get_dummies(df, columns=['island'])\n"
      ],
      "metadata": {
        "id": "5u5m2MJHcrT2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# List of numerical features\n",
        "numerical_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n"
      ],
      "metadata": {
        "id": "X-iEvBlqctHr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop('species', axis=1)\n",
        "y = df['species']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "PUQBKX6VcuM5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['newton-cg', 'lbfgs', 'sag', 'saga']\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(\n",
        "        solver=solver, multi_class='multinomial', max_iter=1000\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = {'Accuracy': acc, 'Training Time (s)': training_time}\n",
        "    print(f\"Solver: {solver}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le_species.classes_))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNDnPN5PcvIq",
        "outputId": "fac84493-b53c-4276-f8de-78cbf47b95c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: newton-cg\n",
            "Accuracy: 1.0000\n",
            "Training Time: 0.0211 seconds\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      1.00      1.00        29\n",
            "   Chinstrap       1.00      1.00      1.00        14\n",
            "      Gentoo       1.00      1.00      1.00        24\n",
            "\n",
            "    accuracy                           1.00        67\n",
            "   macro avg       1.00      1.00      1.00        67\n",
            "weighted avg       1.00      1.00      1.00        67\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29  0  0]\n",
            " [ 0 14  0]\n",
            " [ 0  0 24]]\n",
            "--------------------------------------------------\n",
            "Solver: lbfgs\n",
            "Accuracy: 1.0000\n",
            "Training Time: 0.0165 seconds\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      1.00      1.00        29\n",
            "   Chinstrap       1.00      1.00      1.00        14\n",
            "      Gentoo       1.00      1.00      1.00        24\n",
            "\n",
            "    accuracy                           1.00        67\n",
            "   macro avg       1.00      1.00      1.00        67\n",
            "weighted avg       1.00      1.00      1.00        67\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29  0  0]\n",
            " [ 0 14  0]\n",
            " [ 0  0 24]]\n",
            "--------------------------------------------------\n",
            "Solver: sag\n",
            "Accuracy: 1.0000\n",
            "Training Time: 0.0096 seconds\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      1.00      1.00        29\n",
            "   Chinstrap       1.00      1.00      1.00        14\n",
            "      Gentoo       1.00      1.00      1.00        24\n",
            "\n",
            "    accuracy                           1.00        67\n",
            "   macro avg       1.00      1.00      1.00        67\n",
            "weighted avg       1.00      1.00      1.00        67\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29  0  0]\n",
            " [ 0 14  0]\n",
            " [ 0  0 24]]\n",
            "--------------------------------------------------\n",
            "Solver: saga\n",
            "Accuracy: 1.0000\n",
            "Training Time: 0.0096 seconds\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Adelie       1.00      1.00      1.00        29\n",
            "   Chinstrap       1.00      1.00      1.00        14\n",
            "      Gentoo       1.00      1.00      1.00        24\n",
            "\n",
            "    accuracy                           1.00        67\n",
            "   macro avg       1.00      1.00      1.00        67\n",
            "weighted avg       1.00      1.00      1.00        67\n",
            "\n",
            "Confusion Matrix:\n",
            "[[29  0  0]\n",
            " [ 0 14  0]\n",
            " [ 0  0 24]]\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the results\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Comparison of Different Solvers:\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K27XEY-cwSd",
        "outputId": "a4190a4b-b3e5-423b-8f4e-f60a5be254cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Different Solvers:\n",
            "           Accuracy  Training Time (s)\n",
            "newton-cg       1.0           0.021144\n",
            "lbfgs           1.0           0.016504\n",
            "sag             1.0           0.009618\n",
            "saga            1.0           0.009632\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}